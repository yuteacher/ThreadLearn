6.5.3 工作原理
	该程序模拟了使用多个网络爬虫进行网页索引的场景。网络爬虫是这样一个程序:它使用网页地址打开一个网页，索引该网页内容，尝试访问该页面包含的所有链接，并且也索引这些链接页面。刚开始，我们定义了
一个包含不同网页URL的字典。该字典模拟了包含其他页面链接的网页。该实现非常简单，并不关心索引已经访问过的页面，但正因为它如此简单我们才可以关注并行工作负载。接着创建了一个并发包，其中包含爬虫任
务。我们创建了四个爬虫，并且给每个爬虫都提供了一个不同的网站根URL。然后等待所有爬虫完成工作。现在每个爬虫开始检索提供给它的网站 URL。我们通过等待一个随机事件来模拟网络I/0处理。如果页面包含的 
URL越多，爬虫向包中放入的任务也会越多。然后检查包中是否还有任何需要爬虫处理的任务，如果没有说明爬虫完成了工作。如果检查前四个根 URL后的第一行输出内容，我们将看到被爬虫N放置的任务通常会被同一个
爬虫处理。然而，接下来的行则会不同。这是因为ConcurrentBag 内部针对多个线程既可以添加元素又可以删除元素的场景进行了优化。实现方式是每个线程使用自己的本地队列的元素，所以使用该队列时无需任何锁。
只有当本地队列中没有任何元素时，我们才执行一些锁定操作并尝试从其他线程的本地队列中“偷取”工作。这种行为有助于在所有工作者间分发工作并避免使用锁。